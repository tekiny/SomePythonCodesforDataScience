{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This a simple tokenizer using TweetTokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"https://www.nltk.org This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tokenizer keep the symbols like separate;$3.88 ==> '$' and '3.88'\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True) # to keep @remy  make strip_handles='False'  \n",
    "s1 = '@remy: This is #waaaaayyyy too much for you!!!!!!'\n",
    "s2=\"#Good muffins cost  $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "tknzr.tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr.tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep the symbols and word tokens together ; $0.5 tokenized as $0.5 and .!!! remains same\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "s = \" https://nltk.org #Good muffins $0.5 cost $3.88\\nin 'New York' the . $$2  15% Please'll buy me\\ntwo of them.\\nThanks.!!! I've isn't you are costs \"\n",
    "#s='are you happy with your health here are 3 powerful ways to transform your physical transforming mental and emotional state'\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different versions of tokeinizations some of them removes contractions, some removes the tokens start with http etc \n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "tknzr = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#for stopw in set(ENGLISH_STOP_WORDS.words('english')):\n",
    "s=s.lower()\n",
    "for stopw in (ENGLISH_STOP_WORDS):\n",
    "    s = re.sub(r'\\b%s\\b' % stopw, '', s)\n",
    "#s01=re.sub('[^0-9a-zA-Z$#\\\\s\\\\b]+(\\.?()(?![0-9]+))|https?://[^\\s]*','',s)\n",
    "#  add Remove letters with aposthropes\n",
    "#s01=re.sub('[^0-9a-zA-Z$#\\\\s\\\\b]+(\\.?()(?![0-9]+))|(?<=n{1})\\'[a-z]{1,2}\\b|https?://[^\\s]*','',s)\n",
    "regexp = re.compile('\\\\n+|\\'s|\\'ll|\\'ve|n\\'t|\\'re|\\'d|\\'m|[^0-9a-zA-Z$#%@\\\\s\\\\b]+(\\.?()(?![0-9]+))|https?://[^\\s]*')\n",
    "         re.compile('\\\\n+|\\'s|\\'ll|\\'ve|n\\'t|\\'re|\\'d|\\'m|[^0-9a-zA-Z$#%@\\-\\\\s\\\\b]+(\\.?()(?![0-9]+))|https?://[^\\s]*')\n",
    "s01=re.sub(regexp,' ',s)\n",
    "\n",
    "\n",
    "#Correct# s01=re.sub('\\'s|\\'ll|\\'ve|n\\'t|\\'re|\\'d|\\'m|\\\\n|[^0-9a-zA-Z$#\\\\s\\\\b]+(\\.?()(?![0-9]+))|https?://[^\\s]*',' ',s)\n",
    "\n",
    "#also correct s01=re.sub('(?<=\\w)\\'\\w+|[^0-9a-zA-Z$#\\\\s\\\\b]+(\\.?()(?![0-9]+))|https?://[^\\s]*','',s)\n",
    "\n",
    "#s01=re.sub('\\'s|\\'ll|\\'ve|n\\'t|\\'re|\\'d|\\'m', '', s)\n",
    "print(s01)\n",
    "#regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "#correct# x=regexp_tokenize(s01, pattern='\\w+|\\$[\\d\\.]+|\\S+')+regexp_tokenize(s01,pattern='\\$+?')\n",
    "x=regexp_tokenize(s01, pattern='\\d+\\.\\d+|\\w\\w+|\\S+')+regexp_tokenize(s01,pattern='\\$+?')\n",
    "s01_cln=s01.split()\n",
    "s01_cln_words = [lemmatizer.lemmatize(w) for w in s01_cln]\n",
    "print(\"******tweet tokenized and lemmatized after re.sub\",s01_cln_words)\n",
    "#second alternate# x=regexp_tokenize(s01, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "#y=re.findall('\\$+',s01)\n",
    "print(\"*******after my tokenize\",x)\n",
    "tk=tknzr.tokenize(s01)\n",
    "tk1=tknzr.tokenize(s)\n",
    "print(\"*****only tweet tokenize:\",tk1)\n",
    "print(\"*****tweet tokenize after re.sub\",tk)\n",
    "type(tk)\n",
    "x1=TfidfVectorizer(tk)\n",
    "x2=TfidfVectorizer(x)\n",
    "x3=TfidfVectorizer(s)\n",
    "print(\"*****tf idf with tweet tokenize:\",x1)\n",
    "print(\"****tf idf with my tokenize:\",x2)\n",
    "print(\"****only tf idf:\",x3)\n",
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "#regexp_tokenize(s, pattern='\\\\b\\\\w\\\\w+\\\\b')\n",
    "#wordpunct_tokenize(s)\n",
    "#blankline_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s01_cln_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(x1)\n",
    "print(x1.transform)\n",
    "#type(tk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from nltk.internals import find_jar, config_java, java, _java_options\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "_JAR = 'stanford-postagger.jar'\n",
    "\n",
    "_stanford_url = 'https://nlp.stanford.edu/software/tokenizer.shtml'\n",
    "stfd=StanfordTokenizer(path_to_jar=None, encoding='utf8', options=None, verbose=False, java_options='-mx1000m')\n",
    "s02 = \"#Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\"\n",
    "stfd.tokenize(s02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
